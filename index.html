
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}

/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Opti-Acoustic Semantic SLAM for Underwater Environments"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    </head>

 <body>
<div class="topnav" id="myTopnav">
    <div>
        <a href="https://marinerobotics.mit.edu/" ><strong>MIT Marine Robotics Group</strong></a>
    </div>
</div>
<div class="container">
    <div class="paper-title">
    <h1> 
        Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments     
    </h1>
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://www.linkedin.com/in/kurran-singh/">Kurran Singh<sup>1</sup></a>,
                <a href="https://jungseokhong.com/">Jungseok Hong<sup>1</sup></a>,
                <a href="https://nrr.mit.edu/about">Nicholas R. Rypkema<sup>2</sup></a>
                <a href="https://meche.mit.edu/people/faculty/JLEONARD@MIT.EDU">John J. Leonard<sup>1</sup></a>
            </div>
        </center>
        <center>
            <div class="affiliations">
                <span><sup>1</sup>MIT CSAIL, <sup>2</sup>WHOI</span><br/>
                {singhk,jungseok,jleonard}@mit.edu, {nrypkema@whoi}.edu
            </div>
        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="http://arxiv.org/abs/2403.12837">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="https://www.dropbox.com/scl/fo/aasnt0oovh6rim4p9obb1/h?rlkey=rxnocuna9tpscm9jfgeld04pj&st=okid6egl&dl=0">
                    <span class="material-icons"> folder </span>
                    Data
                </a>
            </div>
        </div></div>
    </div>

    <!-- <style>
        .video-container {
          width: 100%; /* Each video takes up 1/3 of the container width */
          float: left; /* Float videos left to display them horizontally */
          box-sizing: border-box; /* Include padding and border in the element's total width and height */
          padding: 10px; /* Add padding around each video */
        }
      
        video {
          width: 100%; /* Make sure the video takes up 100% of its container width */
          height: auto; /* Automatically adjust height based on width to maintain aspect ratio */
        }
      </style>


      <div class="video-container">
        <video autoplay loop muted>
          <source src="assets/open_set_trimmed.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
       -->
    <center>
    <figure style="width: 100%;">
            <a>
                <img width="80%" src="assets/map_with_real_image.png"> 
            </a>
            <p class="caption" style="margin-bottom: 24px;"><br>
                The upper portion of the image illustrates an underwater vehicle navigating through an underwater environment where various objects are placed.
                The trajectory map below shows the estimated trajectory of the vehicle and the location of objects encountered during the mission. The colors are randomly
                assigned based on semantics. Black lines represent correct matches, a red dashed line shows false positives due to an error from feature embedding, and
                a cyan dashed line is a misaligned object match in the map due to an error from range values from the sonar sensor.
            </p>
    </figure>
    </center>
    <section id="news">
        <hr>
        <h2>News</h2>
        <div class="row">
            <div><span class="material-icons"> event </span> [July 2024] Paper accepted to IROS 2024! </div>
            <div><span class="material-icons"> event </span> [March 2024] <a href="https://www.dropbox.com/scl/fo/aasnt0oovh6rim4p9obb1/h?rlkey=rxnocuna9tpscm9jfgeld04pj&st=okid6egl&dl=0">Labeled training data</a> released!</div>
            <div><span class="material-icons"> event </span> [March 2024] <a href="http://arxiv.org/abs/2403.12837">Paper</a> uploaded to arxiv!</div>
            <div><span class="material-icons"> event </span> [March 2024] <a href="https://kurransingh.github.io/underwater-semantic-slam/">Project page</a> released!</div>


        </div>
    </section>
    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                Despite recent advances in semantic Simultaneous
                Localization and Mapping (SLAM) for terrestrial and aerial
                applications, underwater semantic SLAM remains an open and
                largely unaddressed research problem due to the unique sensing
                modalities and the differing object classes. This paper presents
                a semantic SLAM method for underwater environments that
                can identify, localize, classify, and map a wide variety of
                marine objects without a priori knowledge of the scene’s
                object makeup. The method performs unsupervised object
                segmentation and object-level feature aggregation, and then
                uses opti-acoustic sensor fusion for object localization with
                probabilistic data association and graphical models for back-
                end inference. Indoor and outdoor underwater datasets with a
                wide variety of objects and challenging acoustic and lighting
                conditions are collected for evaluation. The datasets are made
                publicly available. Quantitative and qualitative results show the
                proposed method achieves reduced trajectory error compared
                to baseline methods, and is also able to obtain comparable map
                accuracy to a baseline closed-set method that requires hand-
                labeled data of all objects in the scene.
            </p>
        </div>
        <br>

    <center>
        <iframe width="840" height="472.5" src="https://www.youtube.com/embed/PIkTw6k4da4?si=JO5Umnla0KLwX8DL" title="YouTube video player"
                frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                allowfullscreen></iframe>
    </center>
    <br>
    </section>
    <section id="method"/>
        <hr>
        <h2>Method</h2>
        <center>
            <figure style="width: 100%;">
                    <a>
                        <img width="80%" src="assets/sys_diagram.png"> 
                    </a>
                    <p class="caption" style="margin-bottom: 24px;"><br>
                        The pipeline receives sensor inputs from a multibeam sonar, a monocular camera, an IMU, a DVL, and a pressure sensor. (1) Segment objects from
                        optical images and extract features for each segmentation mask. Features are projected into a fixed dimension for the data association process. (2) Given
                        the segmentation mask, its pixel centroid is used to find a corresponding range from sonar returns and estimate a 3D position of the object. (3) In parallel,
                        sensor readings from IMU, DVL, and pressure sensor are used to estimate odometry. Outputs from (1)-(3) are used to build a factor graph and optimize
                        it via iSAM2 to obtain map and trajectory estimates.
                    </p>
            </figure>

    </section>
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div class="download-thumb">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="https://kurransingh.github.io/underwater-semantic-slam/"><img class="screenshot" src="assets/title_page_scrnsht.png"></a>
            </div>
        </div>
            <div class="paper-stuff">
                <p><b>Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments</b></p>
                <p>Kurran Singh and Jungseok Hong and Nicholas R. Rypkema and John J. Leonard</p>
                <div><span class="material-icons"> description </span><a href="http://arxiv.org/abs/2403.12837"> arXiv version</a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/singh2024optiacousticslam.bib"> BibTeX</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href="https://www.dropbox.com/scl/fo/aasnt0oovh6rim4p9obb1/h?rlkey=rxnocuna9tpscm9jfgeld04pj&st=okid6egl&dl=0"> Data</a></div>
            </div>
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{singh2024opensetslam,
    title=Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments,
    author={Kurran Singh and Jungseok Hong and Nicholas R. Rypkema and John J. Leonard},
    booktitle={arxiv Preprint},
    year={2024}
}</code></pre>
    </section>


    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
    </section>
</div>
</body>
</html>
